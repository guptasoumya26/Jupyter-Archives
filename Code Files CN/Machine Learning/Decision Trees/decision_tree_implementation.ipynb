{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE IMPLEMENTATION FROM SCRATCH\n",
    "### IRIS DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "#to split the dataset into test and train\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree class\n",
    "class Decision_Tree(object):\n",
    "    \n",
    "    def __init__(self, max_depth, min_size):\n",
    "# maximum depth of tree and minimum number of split as input to the class\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.labels=[]\n",
    "        \n",
    "\n",
    "\n",
    "    def split_info(self,groups):\n",
    "        total=0\n",
    "        for group in groups:\n",
    "            total=total+len(group)\n",
    "            \n",
    "        split_info=0.0\n",
    "        for group in groups:\n",
    "            probability=float(len(group)/total)\n",
    "            if probability!=0.0:\n",
    "                split_info-= probability*math.log(probability)\n",
    "            \n",
    "        return split_info\n",
    " \n",
    "\n",
    "\n",
    " # calculates entropy of group\n",
    "    def entropy(self,groups):\n",
    "        entries = 0\n",
    "        labels = {}\n",
    "        for group in groups:\n",
    "            entries=entries+len(group)\n",
    "            for row in group:\n",
    "                label = row[-1]\n",
    "                if label not in labels.keys():\n",
    "                    labels[label] = 0\n",
    "                labels[label] += 1\n",
    "                \n",
    "                \n",
    "        entropy = 0.0\n",
    "        for key in labels:\n",
    "            probability = float(labels[key])/entries\n",
    "            if probability!=0.0:\n",
    "                entropy -= probability * math.log(probability)\n",
    "        return float(entropy)  \n",
    "        \n",
    "        \n",
    "# calculates entropy of singles     \n",
    "    def entropy_single(self,group):\n",
    "        labels = {}\n",
    "        entries=len(group)\n",
    "        for row in group:\n",
    "                label = row[-1]\n",
    "                if label not in labels.keys():\n",
    "                    labels[label] = 0\n",
    "                labels[label] += 1\n",
    "                \n",
    "        entropy = 0.0       \n",
    "        for key in labels:\n",
    "            probability = float(labels[key])/entries\n",
    "            if probability!=0.0:\n",
    "                entropy -= probability * math.log(probability)\n",
    "        return float(entropy)  \n",
    "        \n",
    "        \n",
    "        \n",
    "#claculates informatio gain     \n",
    "    def info_gain(self,groups):\n",
    "        total=0\n",
    "        for group in groups:\n",
    "            total=total+len(group)\n",
    "        info_gain = self.entropy(groups)\n",
    "        \n",
    "        for group in groups:\n",
    "#             entropy= self.entropy(group)\n",
    "            labels = {}\n",
    "            for row in group:\n",
    "                label = row[-1]\n",
    "                if label not in labels.keys():\n",
    "                    labels[label] = 0\n",
    "                labels[label] += 1\n",
    "            entropy=0.0    \n",
    "            for key in labels:\n",
    "                probability=float(labels[key]/len(group))\n",
    "                if probability!=0.0:\n",
    "                    entopy=entropy-probability*math.log(probability)\n",
    "            info_gain=info_gain-float(entropy*(float)(len(group)/total))\n",
    "                                         \n",
    "        return info_gain\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#calculates gain ratio\n",
    "    def gain_ratio(self,groups):\n",
    "        split_info=self.split_info(groups)\n",
    "        info_gain=self.info_gain(groups)\n",
    "        if split_info==0:\n",
    "            return 0.0\n",
    "        gain_ratio=float(info_gain/split_info)\n",
    "        return gain_ratio\n",
    "        \n",
    "\n",
    "    \n",
    " # Split a dataset based on an feature and its value\n",
    "    def test_split(self,index, value, xy_train):\n",
    "        left, right = list(), list()\n",
    "        for row in xy_train:\n",
    "            if row[index] <= value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Select the best split point for a dataset\n",
    "    def get_split(self,xy_train):\n",
    "    #class_values has all distinct labels value\n",
    "        class_values = [set(row[-1] for row in xy_train)]\n",
    "\n",
    "        best_index, best_value, best_score, best_groups = -1, -1, -1, None\n",
    "        #call test_split for every value of every feature \n",
    "        #and find the best split with maximum information gain\n",
    "        for index in range(len(xy_train[0])-1):\n",
    "            for row in xy_train:\n",
    "                groups = self.test_split(index, row[index], xy_train)\n",
    "                info_gain = self.info_gain(groups)\n",
    "                if info_gain > best_score:\n",
    "                    best_index, best_value, best_score, best_groups = index, row[index], info_gain, groups\n",
    "\n",
    "        return {'index':best_index, 'value':best_value, 'groups':best_groups}\n",
    "\n",
    "    \n",
    "# Create a terminal node value\n",
    "    def to_terminal(self,group):\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Create child splits for a node or make terminal\n",
    "    def split(self,node,depth):\n",
    "        left, right = node['groups']\n",
    "      \n",
    "        # check for a no split\n",
    "        \n",
    "            \n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right)\n",
    "            return\n",
    "        # check for max depth\n",
    "        \n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
    "            return\n",
    "        \n",
    "        # process left child\n",
    "        if self.entropy_single(left)==0.0:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        \n",
    "        elif len(left) <= self.min_size:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            node['left'] = self.get_split(left)\n",
    "            self.split(node['left'], depth+1)\n",
    "        # process right child\n",
    "        if self.entropy_single(right)==0.0:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        elif len(right) <= self.min_size:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            node['right'] = self.get_split(right)\n",
    "            self.split(node['right'],depth+1)\n",
    "            \n",
    "            \n",
    "            \n",
    "# to build a decision tree\n",
    "    def build_tree(self):\n",
    "        self.root = self.get_split(self.xy_train)\n",
    "        self.split(self.root, 1)\n",
    "        return self.root\n",
    "    #root is the root node of tree\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "#     Print a decision tree\n",
    "    def print_tree(self, node, depth):\n",
    "        \n",
    "        print(\"LEVEL \",depth)\n",
    "        if isinstance(node, dict):\n",
    "     \n",
    "            \n",
    "            \n",
    "            lblctr={}\n",
    "            \n",
    "            for group in node['groups']:\n",
    "                for row in group:\n",
    "                    label=row[-1]\n",
    "                    if label not in lblctr:\n",
    "                        lblctr[label] = 0\n",
    "                    lblctr[label] += 1\n",
    "                \n",
    "            for key,value in lblctr.items():\n",
    "                print(\"Count of \",key ,\" = \",value)\n",
    "#             print(node['groups'])\n",
    "            print(\"entropy is \",self.entropy(node['groups']))\n",
    "#             print(\"split info is \",self.split_info(node['groups']))\n",
    "           \n",
    "            print(\"Splitting on feature \", node['index']+1)\n",
    "            print(\"with gain ratio \",self.gain_ratio(node['groups']))\n",
    "            print()\n",
    "            \n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"Reached Leaf node \" )\n",
    "            print(\"predicted value at this node is \",node)\n",
    "            print()\n",
    "            \n",
    "            \n",
    "\n",
    "   \n",
    "\n",
    " #  function to call print_tree\n",
    "    def display_tree(self):\n",
    "        print(\"The Tree formed is as follows:\\n\")\n",
    "        self.print_tree(self.root,0)\n",
    "       \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "# Make a prediction with a decision tree\n",
    "    def predict(self,node, row):\n",
    "        if row[node['index']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.predict(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "            \n",
    "            \n",
    "# function to call predict in each row of test data           \n",
    "    def predict_tree(self,x_test):\n",
    "        self.y_pred = np.array([])\n",
    "        for i in x_test:\n",
    "            self.y_pred = np.append(self.y_pred,self.predict(self.root,i))\n",
    "            \n",
    "        return self.y_pred\n",
    "        \n",
    "        \n",
    "        \n",
    "# fit function will merge x_train and y_train and call build_tree          \n",
    "    def fit(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.xy_train = np.column_stack((X, y))\n",
    "        self.labels=[set(row[-1] for row in self.xy_train)]\n",
    "#         print(self.labels)\n",
    "        self.build_tree()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = Decision_Tree(max_depth=3,min_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tree formed is as follows:\n",
      "\n",
      "LEVEL  0\n",
      "Count of  1.0  =  34\n",
      "Count of  0.0  =  37\n",
      "Count of  2.0  =  41\n",
      "entropy is  1.0956714129052516\n",
      "Splitting on feature  1\n",
      "with gain ratio  1.5987869524252456\n",
      "\n",
      "LEVEL  1\n",
      "Count of  1.0  =  21\n",
      "Count of  0.0  =  37\n",
      "Count of  2.0  =  5\n",
      "entropy is  0.879862924451871\n",
      "Splitting on feature  1\n",
      "with gain ratio  0.0\n",
      "\n",
      "LEVEL  2\n",
      "Reached Leaf node \n",
      "predicted value at this node is  0.0\n",
      "\n",
      "LEVEL  2\n",
      "Reached Leaf node \n",
      "predicted value at this node is  0.0\n",
      "\n",
      "LEVEL  1\n",
      "Count of  2.0  =  36\n",
      "Count of  1.0  =  13\n",
      "entropy is  0.5785341056326687\n",
      "Splitting on feature  1\n",
      "with gain ratio  0.9158449197401396\n",
      "\n",
      "LEVEL  2\n",
      "Count of  2.0  =  22\n",
      "Count of  1.0  =  11\n",
      "entropy is  0.6365141682948128\n",
      "Splitting on feature  1\n",
      "with gain ratio  0.0\n",
      "\n",
      "LEVEL  3\n",
      "Reached Leaf node \n",
      "predicted value at this node is  2.0\n",
      "\n",
      "LEVEL  3\n",
      "Reached Leaf node \n",
      "predicted value at this node is  2.0\n",
      "\n",
      "LEVEL  2\n",
      "Count of  2.0  =  14\n",
      "Count of  1.0  =  2\n",
      "entropy is  0.37677016125643675\n",
      "Splitting on feature  1\n",
      "with gain ratio  0.6700099840137842\n",
      "\n",
      "LEVEL  3\n",
      "Reached Leaf node \n",
      "predicted value at this node is  2.0\n",
      "\n",
      "LEVEL  3\n",
      "Reached Leaf node \n",
      "predicted value at this node is  2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.display_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=dt.predict_tree(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 0., 2., 0., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 0., 2.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 2., 0., 0., 2.,\n",
       "       2., 0., 0., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted values\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5263157894736842"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
