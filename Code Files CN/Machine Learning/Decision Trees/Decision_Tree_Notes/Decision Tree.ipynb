{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a graph that uses a branching method to illustrate every possible outcome of a decision. Lets start by taking a example of OR of two variables X1 and X2. The decison tree for the same is shown below :\n",
    "<br>\n",
    "<img src=\"OR_DT.png\">\n",
    "We have total four possible combinations as shown in the truth table. So at the beginning we have 2 True and 2 False outcomes to start with. The first condition we check for is the value of X1. If X1 is true then we are sure that the result will be true and hence we arrive at 2 true and no false. Note that ,we only consider those rows of the table which are having X1 as true i.e. the first and the third row in the current example .That is why we have 2 total outcomes out of which 2 are true and none is false. This node is having a definite answer.If we arrive at this node there is no confusion as to which what our tree should return, therefore these are called <b>pure nodes</b>.<br>\n",
    "If X1 is false then the answer is dependant on the value of X2. If X2 is false the answer is false and if the answer is true the final result is true. We have three pure nodes here as shown in the pictorial representation of the decision tree.\n",
    "Now if we are given any new data for prediction we just need to run it through our tree. For example if we get A and B as two values for testing. We check first if A is true. If A is true we have true as our answer else we need to check for the value of B. If B is true then our answer agin is true else we have false as the final outcome.\n",
    "Lets consider another example which is not as straight forward as the previous one. Suppose we need to predict that if a person will get interview call or not based on some factors. There can be many factors but for simplicity lets consider that we focus on the level of projects, good inten and whether the person is from top 50 colleges or not.<br>\n",
    "Unlike the previous example in which we just picked X1 for the first decision, it is quite arguable that which factor should we pick first here. Lets assume that we start by picking whether the person has done a good internship or not(which is in the form of true or false).\n",
    "<img src=\"DT_interview.png\">\n",
    "<br>\n",
    "Similarly we continue to break down the nodes further on the basis of the features which are still left(type of college and projects in this case ).<br/>\n",
    "Consider a case where we have used up all the features and still we do not arrive at a pure node. Cases like this can surely be true. For eg: we may have three students lacking in all the three fields but still one gets the interview call and the other two do not. In such a case, our node which corresponds to \"no\" at each of the above three decisions cannot give a pure no as answer i.e. it is not a pure node. What should we do then? As we do not have anymore features left to judge on, we can simply favour the majority class of the node.<br>\n",
    "Therefore, there are two cases when we need to stop with the breaking of nodes into subparts, those are :<br>\n",
    "1) When we have a pure node, there is no need to proceed further.<br>\n",
    "2) When all the features are used and we dont have any other feature present.\n",
    "<br>\n",
    "Depending upon which factor we pick to split on we can have different Decision trees and their accuracy will also vary. We will discuss how to pick a decent tree in the next section. What we should note is that till now we have only considered the examples where we have binary( true or false ) as outcomes. If a partucular feature has n different values, we may have to break a node into n subnodes. If a feature has all unique entry then we reach the pure nodes in one step but for many cases that is useless. For example, dividing a class of students on the basis of their roll number into subnodes may result in all pure nodes but it is of no use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a decision tree involves deciding on which feature to choose and what condition to use on splitting, alongwith knowing when to stop.\n",
    "In the first split on the root, all features are considered and the data points are divided into groups based on this split. Lets suppose, we have n features. Then we will be having n candidate splits at the first level. Now, we will calculate how much accuracy each split will cost us, using a function. The split(feature) which results in maximum accuracy is choosen at this level and data points are divided into child nodes according to that feature only. The child nodes formed are recursively divided into deeper levels, resulting in formation of the entire tree.\n",
    "\n",
    "In the case we have n features, then we can possibly make exponential number of decision trees. It is categorised into NP-HARD Problem. For finding out the best tree all possible combinations of tree possible should be taken care off. So, we are interested here to find out the good tree and not the best one. Using the GREEDY approach, we will try to lower the cost (and also maximize the accuracy) and according to this, build a good decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose feature to split on (on the basis of accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the same example of the student applying for an internship in college will get a call for the interview or not. Considering total 50 students, we will pick up all the features one by one and see how many mistakes we make at each level, taking the decision on the basis of majority. Majority decision means that if we have <b>10 YES and 40 NO</b> in particular node, then we will take our decision as NO for that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"feature_1.jpeg\" width=\"700\">\n",
    "\n",
    "At the root node, we will take our decision as NO according to majority, then we make a total of 10 mistakes. So, it is represented as 10/50(10 out of 50) are wrong decisions. <br/>\n",
    "Furthur, if we split the data points on the feature \"Great Intern\", then left node represents students who are great interns, total of 8 students, out of which 5 got a call for interview and 3 didn't. Right node similarly reprents students who are not great interns, total of 42 students, and again out of which 5 got a call for interview and 37 didn't. So, we take our decision as YES for left side child node and NO for side side child node (according to majority only). And we make a total of 8(3+5) mistakes at this level, which actually means that before using this features, if we took decision at root node only we make 10 mistakes in our decision, while after splitting on this feature we make a total of 8 mistakes at this level. Since, we are making less number of mistakes in our decision at lower level, so it will be benificial and serve as an advantage in decision making if we make a split at this level.\n",
    "\n",
    "<img src=\"feature_2.jpeg\" width=\"700\">\n",
    "<br/>\n",
    "Similarly, here we are making 10 mistakes at the root level, but after splitting we are making a total of 6(2+4) mistakes in decision at the lower level, and hence this split is also favoured at this step of splitting.\n",
    "\n",
    "<img src=\"feature_3.jpeg\" width=\"700\">\n",
    "<br/>\n",
    "Here, we are making total of 10 mistakes at root level, but after splitting also we are making 10(5+5) mistakes, so we might possibly avoid split using this feature.\n",
    "\n",
    "<img src=\"tree_legends.jpeg\" width=\"700\">\n",
    "<br/>\n",
    "We will make a split using that feature by which number of mistakes made at the lower level of the tree gets reduced after a split is performed using that feature. Therefore, here we will make a split using second feature i.e. <b>Top College</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Handle discrete and continuous value features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have discrete value feature, say labelled data for example gender of a person we have Male and Females. Now, making a split on gender of person results in 2 child nodes, one for males and other for females.\n",
    "<br/>\n",
    "Consider continuous valued features, say salary. Every person has different salary and values are spread over a wide range. If we have to make a split on Salary, then an option is to make different child node for every different value of salary we obtain. But unfortunately, it will result in large overfitting of data.\n",
    "<br/>\n",
    "So, to avoid this difficulty for continuous value features. We follow the procedure mentioned to achieve the better split using this feature.\n",
    "\n",
    "1. Spread all the salaries(values for feature choosen) on the straight line from lowest to highest order.\n",
    "2. Split the data according to mid point values, taking all the pairwise adjacent points.\n",
    "3. Take the salary value for that particular split that results in maximum accuracy or minimum mistakes made while making the decision. \n",
    "\n",
    "Example, we have 4 value of salaries, 5000, 10000, 20000 and 50000.\n",
    "We take the mid point of all these which comes out to be 7500, 15000 and 35000.\n",
    "We will make a split on all these salary values one by one, first on 7500 salary, which means people with salary less than 7500 come on left side and all others on the right side. \n",
    "Similarly, doing for salary values 15000 and 35000.\n",
    "We choose that particular salary value to make a split at this level which results in maximum accuracy.\n",
    "\n",
    "This process is followed for making a binary split for the continuous value features. We can choose salary feature again at the deeper level and make a split again using this feature below, with decreased range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Code using sklearn decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-28fb4a6ca2e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading IRIS dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using decision tree classifier of sklearn\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(df,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the decision tree formed using pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing tree alongwith class names\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True, feature_names = iris.feature_names, class_names = iris.target_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFORMATION GAIN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information gain is based on the decrease in entropy after a dataset is split on an attribute(feature). Constructing a decision tree is all about finding attribute that returns the highest information gain (i.e., the most homogeneous branches).\n",
    "\n",
    "Entropy is basically the randomness in the decision tree . For example if we have pure nodes(having only one class) we say that the entropy is minimum,on the contrary if we have 50% of class A and 50% of class B in a node we say that the entropy is maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"info_gain_3.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing [Entropy(old) - Entropy(new)] is information gain as we are going towards more pure or homogeneous nodes. We take weighted average of the entropies of two nodes to get the resultant entropy at the level .For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"info_gain_2.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a situation where we are predicting whether or not a person will get loan or not. If we split our data on the basis of name, then we will be getting lowest entropy and pure nodes, but that would majorly result in one node having one data point and overfitting will take place. Therefore Information Gain cannot be the only criteria on the basis of which we make our decision to split, rather we should also take in consideration the number of resulting nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"info_gain_4.jpeg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Split number determines the degree of split i.e more the number of split more will be the split number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"info_gain_5.jpeg\" width=\"700\">\n",
    " <br/>\n",
    " <img src=\"info_gain_6.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we define our measure on the basis of which we will split on an attribute(or feature) considering both Information gain and Split number . <h4>Gain Ratio = [Information Gain / Split Number].</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More the gain ratio more favorable is the split ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Gini Index</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Index is another measure on the basis of which we decide on which feature to split on. This is used internally in the inbuild Decision tree in Sklearn library ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"gini_1.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Index favors larger partitions. It uses squared proportion of classes.\n",
    "For a perfectly classified i.e pure node ,Gini Index would be zero.\n",
    "We want a variable split that has a low Gini Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gini_2.jpeg\" width=\"700\"/>\n",
    " <br/>\n",
    " <img src=\"gini_3.jpeg\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above shown split we have moved from GI 0.6666 to GI of 0.3333 . A low value of Gini Index is desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally whenever we are using Gini Index as a measure for split we do binary split . Inbuild Decision tress also uses Gini Index ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning is a technique in machine learning that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.<br>\n",
    "Consider the two D-Trees shown below:<br>\n",
    "<img src=\"pruning_one.png\">\n",
    "As we can see , the first tree is less complex than the second tree. Complexity can increase due to Depth as well as the nodes at a level. We cannot decide just on the basis of Depth or just on the basis of number of nodes at a level.<br>\n",
    "We add to our original cost a parameter lamda(位) so that <br><br>\n",
    "<b>Cost = Error_function(Training data) + 位*L(T)</b>\n",
    "<br>where, 位=controlling factor and L(T)=no. of leaf nodes.<br>\n",
    "We start from the bottom of the tree and check for each split. As we encounter a split we decide if we want to keep that split or if we should keep the parent node of that split instead based upon the entropy and cost function of both the situations. If we decide to keep the split, we also have to keep all the splits above that particular split.\n",
    "<br>Consider the example shown below of a D-Tree of getting an interview call based upon good project , great intern and the type of college.\n",
    "<img src=\"DT_pruning.png\"><br>\n",
    "without pruning we have our cost as 0.02 + 0.05 = 0.07 ( taking 位 as 0.01 ans 1/50 from error function )\n",
    "<br>After first pruning we have 0.02+0.04 = 0.06, therefore we remove the first split and do pruning.\n",
    "<br>Now if we undo the second split also i.e. do pruning 2nd time as shown above, we get cost = 0.12+0.03=0.15 . Here error has increased too much, therefore, we should keep the split as it is. Because we are keeping this split we need to keep all the splits above this also."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
